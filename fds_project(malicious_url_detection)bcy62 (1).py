# -*- coding: utf-8 -*-
"""FDS_Project(Malicious URL Detection)BCY62.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ja7LF7z7InBkTO7MNW05kR8h804LincJ

# FDS Project - Malicious URL's Detection using classification Algorithms
  Name  : P. Hima Bindu     
  Roll No : 2022BCY0062

# Importing required modules
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import re

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/Colab Notebooks')

"""# Import DataSet --- Phishing links vs legitimate sites dataset from kaggle

"""

data = pd.read_csv('malicious_phish.csv')

"""# Exploratory data analysis"""

data.shape

"""This dataset contains 651191 instances and 2 variables

"""

data.head()

data.info()

"""This dataset contains categorical variables

Now check for null values in the dataset
"""

data.isnull().sum()

"""As we can see, there are no null values present in this dataset

# Label Encoding of Target column - (type)
"""

from sklearn.preprocessing import LabelEncoder

# Intialize the LabelEncoder
label_encoder = LabelEncoder()

# fit and transform the 'type' column
data['type_encoded'] = label_encoder.fit_transform(data['type'])

# Display the mapping of labels
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print("Label Mapping: ", label_mapping)

# Display the first few rows
data.head()

data.info()

data['type_encoded'].value_counts()

"""# Checking for Duplicate url's"""

duplicate_urls = data[data.duplicated(subset='url')]
print("Number of duplicated URLs:", len(duplicate_urls))

unique_urls_count = data['url'].nunique()
print("Number of unique URLs:", unique_urls_count)

# Display unique URLs (optional: view the first few unique URLs)
unique_urls = data['url'].unique()
print("Unique URLs:", unique_urls[:10])

"""This dataset contains duplicate url's, we cannot directly remove those rows. Investigate duplicate URLs and check their labels. If any duplicates have differing classifications or significant label differences among duplicates, we have to retain those specific instances for further analysis."""

# Check duplicates with different lables

duplicate_different_labels = data[data.duplicated(subset='url', keep=False)].sort_values(by='url')

# Display duplicates with different labels
print("Duplicates with Different Labels:")
print(duplicate_different_labels)

"""1. First, identify duplicates with different labels.
2. Remove them from the dataset
"""

# Identify URLs that have conflicting labels

duplicate_different_labels = data[data.duplicated(subset='url', keep=False)]

conflicting_duplicates = duplicate_different_labels.groupby('url').filter(lambda x: x['type'].nunique() > 1)

# Display the count of conflicting duplicates
print("Number of URLs with conflicting lables:", conflicting_duplicates['url'].nunique())

# Drop rows with conflicting labels
data_cleaned = data.drop(conflicting_duplicates.index)

# Remove remaining duplicated (Same URL, same label)
data_cleaned = data_cleaned.drop_duplicates(subset='url', keep='first').reset_index(drop=True)

# Display the cleaned dataset size

data_cleaned.shape

duplicate_urls = data_cleaned[data_cleaned.duplicated(subset='url')]
print("Number of duplicated URLs:", len(duplicate_urls))

data = data_cleaned

# No more duplicated url's

"""# Visualize data"""

# Setting up plotting style

sns.set(style="whitegrid")

# Plotting the distribution of URL types
plt.figure(figsize=(8,5))
sns.countplot(x='type', data=data, palette='viridis')
plt.title("Distribution of URL Types")
plt.xlabel("URL Type")
plt.ylabel("Count")
plt.show()

"""# Feature Extraction from URLs"""

# 1. URL Length
data['url_length'] = data['url'].apply(len)

# 2. Count of Special Characters (@, -, ., //)
data['count_at'] = data['url'].apply(lambda x: x.count('@'))
data['count_dash'] = data['url'].apply(lambda x: x.count('-'))
data['count_dot'] = data['url'].apply(lambda x: x.count('.'))
data['count_double_dash'] = data['url'].apply(lambda x: x.count('//'))


# 3. Number of Subdomains(Calculate the number of subdomains by counting the number of dots (.))

data['num_subdomains'] = data['url'].apply(lambda x: x.count('.'))


# 4. Presence of IP Address in URL
# (Check if the URL contains an IP address instead of a domain name, which is common in phishing URLs.)

data['has_ip'] = data['url'].apply(lambda x: 1 if re.search(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b', x) else 0)


# Presence of HTTPS
data['https'] = data['url'].apply(lambda x: 1 if 'https' in x else 0)


# displaying few rouws to verify features
data.head()

"""# Exploratory Data Analysis (EDA)

Step 1 - Visualize Feature Distributions
"""

# Assuming you have your cleaned and pre-processed DataFrame as 'data_cleaned'
features = ['url_length','count_at', 'count_dash', 'count_dot', 'count_double_dash', 'num_subdomains', 'has_ip', 'https']

# Visualizing feature distributions with histograms and box plots
plt.figure(figsize=(15, 20))  # Adjust figure size to accommodate all plots

for i, feature in enumerate(features):
    # Plotting histogram
    plt.subplot(len(features), 2, 2 * i + 1)  # Creating subplot for histogram
    sns.histplot(data_cleaned[feature], kde=True, bins=30, color='skyblue')
    plt.title(f'Histogram for {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

    # Plotting box plot
    plt.subplot(len(features), 2, 2 * i + 2)  # Creating subplot for box plot
    sns.boxplot(data=data_cleaned, x=feature, palette='viridis')
    plt.title(f'Box Plot for {feature}')
    plt.xlabel(feature)

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

# Function to calculate and count outliers using the IQR method
def detect_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Counting the number of outliers for each feature
for feature in features:
    outliers = detect_outliers(data_cleaned, feature)
    print(f"Number of outliers in {feature}: {len(outliers)}")

"""# Removing Outliers"""

# Define a function to remove outliers based on the IQR method
def remove_outliers_iqr(df, columns):
    for col in columns:
        # Calculate Q1 and Q3 for the column
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        # Calculate the IQR
        IQR = Q3 - Q1
        # Define the lower and upper bounds
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Remove rows that have outliers in this column
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

# List of columns to remove outliers from
outlier_columns = ['url_length','count_at', 'count_dash', 'count_dot', 'count_double_dash', 'num_subdomains', 'has_ip', 'https']

# Apply the function to the data
data_cleaned = remove_outliers_iqr(data, outlier_columns)

# Display the shape of the dataset before and after removing outliers
print("Original dataset shape:", data.shape)
print("Dataset shape after outlier removal:", data_cleaned.shape)

"""# Step 2 - Correlation Analysis"""

data.head()

# Selecting only numeric features for correlation analysis
numeric_features = ['url_length', 'count_at', 'count_dash', 'count_dot', 'count_double_dash', 'num_subdomains', 'has_ip', 'https', 'type_encoded']

# Calculate the correlation matrix
correlation_matrix = data[numeric_features].corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="viridis", fmt=".2f")
plt.title("Correlation Matrix for Numeric Features After Outlier Removal")
plt.show()

"""# Evaluate Feature Importance with a Model"""

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Split data into features and target
X = data[numeric_features].drop(columns=['type_encoded'])
y = data['type_encoded']

# Train a Random Forest model
rf_model = RandomForestClassifier(random_state=0)
rf_model.fit(X, y)

# Get feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(feature_importance)

"""Finalize Feature Selection"""

# Finalizing feature selection based on previous analysis
selected_features = ['url_length', 'num_subdomains', 'count_dash', 'count_dot',
                     'count_double_dash', 'has_ip', 'https']
X = data[selected_features]
y = data['type_encoded']

"""# Split the Data into Training and Testing Sets"""

from sklearn.model_selection import train_test_split

# Split the data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Training different Models

# Train a Random Forest Model
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize and train the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""# Train a Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression

# Initialize and train the Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

# Make predictions
y_pred_logistic = logistic_model.predict(X_test)

# Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_logistic))
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_logistic))

"""# Train a Decision Tree Model"""

from sklearn.tree import DecisionTreeClassifier, plot_tree

# Initialize and train the Decision Tree model
decision_tree = DecisionTreeClassifier(random_state=42)
decision_tree.fit(X_train, y_train)

# Make predictions
y_pred_tree = decision_tree.predict(X_test)

# Evaluate the model
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_tree))
print("Decision Tree Classification Report:\n", classification_report(y_test, y_pred_tree))

"""# Train a Linear Model (Using LinearSVC - Linear Support Vector Classifier)"""

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score

# Initialize and train the LinearSVC model
linear_svc = LinearSVC(random_state=42, max_iter=10000)
linear_svc.fit(X_train, y_train)

# Make predictions
y_pred_linear_svc = linear_svc.predict(X_test)

# Evaluate the model
print("Linear SVC Accuracy:", accuracy_score(y_test, y_pred_linear_svc))
print("Linear SVC Classification Report:\n", classification_report(y_test, y_pred_linear_svc))

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have your data loaded into a pandas DataFrame `data`
# Columns used for feature extraction
X = data[['url_length', 'count_at', 'count_dash', 'count_dot', 'count_double_dash', 'num_subdomains', 'has_ip', 'https']]
y = data['type_encoded']  # Target variable (encoded types)

# Split the dataset into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Linear SVM " : LinearSVC(),
}

# Store accuracy results
accuracy_results = {}

# Train, test and evaluate each model
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = model.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_results[model_name] = accuracy

# Create a bar plot for comparison of accuracy scores
plt.figure(figsize=(10, 6))
sns.barplot(x=list(accuracy_results.keys()), y=list(accuracy_results.values()), palette='coolwarm')
plt.title("Comparison of Model Accuracies")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.show()